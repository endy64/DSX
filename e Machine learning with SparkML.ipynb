{
    "nbformat": 4, 
    "metadata": {
        "language_info": {
            "pygments_lexer": "ipython3", 
            "name": "python", 
            "mimetype": "text/x-python", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }, 
            "version": "3.5.2", 
            "file_extension": ".py", 
            "nbconvert_exporter": "python"
        }, 
        "kernelspec": {
            "language": "python", 
            "name": "python3-spark21", 
            "display_name": "Python 3.5 (Experimental) with Spark 2.1"
        }
    }, 
    "cells": [
        {
            "source": "# Machine Learning with Spark ML\n\n### In this notebook, we will explore machine learning using Spark ML. We will exploit Spark ML's high-level APIs built on top of DataFrames to create and tune machine learning pipelines. Spark ML Pipelines enable combining multiple algorithms into a single pipeline or workflow. We will utilize Spark ML's feature transformers to convert, modify and scale the features that will be used to develop the machine learning model. Finally, we will evaluate and cross validate our model to demonstrate the process of determining a best fit model and load the results in the database.\n\n### We are using machine learning to try to predict records that a human has not seen or vetted before. We will use these predictions to sort the highest priority records for a human to look at. We will use as a training set for the algorithm fake data that has been vetted by an analyst as high, medium or low.\u00b6\n\n### We will use generated travel data that has been examined for patterns of Human Trafficking from DB2 Warehouse to do the machine learning.  We loaded this data in Lab 1.\n\n", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "## Table of contents\n\n1. [Create Version](#version)\n1. [Install Packages](#install)\n1. [Connect to Database](#database)\n1. [Transform the data](#transform)\n1. [Feature Engineering](#engineering)\n1. [Model the data](#model)\n1. [Setup the Pipeline](#pipeline)\n1. [Train the model](#train)\n1. [Evaluate results](#evaluate)\n1. [Hyperparameter Tuning](#tuning)\n1. [Score the records](#score)\n1. [Insert Credentials](#credentials)\n1. [Write Results](#write)\n1. [Create New Version](#version2)\n1. [Schedule Job](#schedule)\n1. [Revert to Version](#revert)\n1. [Even More Help](#help)\n", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "<a id=\"version\"></a>\n## Create Version \n\nSave a version of the notebook by selecting <b>File</b> > <b>Save Version</b> \n<img alt=\"IBM Bluemix.Get started now\" src=\"https://raw.githubusercontent.com/jpatter/LMCO/master/Lab-1/images/FileOptions.PNG\" > or by selecting the <b>Versions</b> icon and selecting <b>Save Version</b>. <img alt=\"IBM Bluemix.Get started now\" src=\"https://raw.githubusercontent.com/jpatter/LMCO/master/Lab-1/images/versions-button.png\" ><br>\nYou can have up to ten (10) versions of a notebook.   Notebook versions are saved in a FIFO manner.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "## Verify Spark version and existence of Spark", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "print('The spark version is {}.'.format(spark.version))", 
            "metadata": {}, 
            "execution_count": 1, 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "The spark version is 2.1.0.\n"
                }
            ], 
            "cell_type": "code"
        }, 
        {
            "source": "<a id=\"install\"></a>\n## Install pixiedust.  With this package we can do some nice visualizations.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "!pip install --trusted-host pypi.python.org JayDeBeApi==0.2.0 --user\n!pip install --trusted-host pypi.python.org --user --upgrade ibmdbpy\n!pip install --trusted-host pypi.python.org --user --upgrade pixiedust", 
            "metadata": {}, 
            "execution_count": 3, 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Requirement already satisfied (use --upgrade to upgrade): JayDeBeApi==0.2.0 in /usr/local/src/conda3_runtime.v20/4.1.1/lib/python3.5/site-packages\nRequirement already satisfied (use --upgrade to upgrade): JPype1 in /usr/local/src/conda3_runtime.v20/4.1.1/lib/python3.5/site-packages (from JayDeBeApi==0.2.0)\nRequirement already up-to-date: ibmdbpy in /usr/local/src/conda3_runtime.v20/4.1.1/lib/python3.5/site-packages\nCollecting six (from ibmdbpy)\n  Downloading six-1.11.0-py2.py3-none-any.whl\nCollecting future (from ibmdbpy)\n  Downloading future-0.16.0.tar.gz (824kB)\n\u001b[K    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 829kB 35.4MB/s ta 0:00:01\n\u001b[?25hCollecting numpy (from ibmdbpy)\n  Downloading numpy-1.13.3-cp35-cp35m-manylinux1_x86_64.whl (16.9MB)\n\u001b[K    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 16.9MB 52.1MB/s ta 0:00:01\n\u001b[?25hCollecting pypyodbc (from ibmdbpy)\n  Downloading pypyodbc-1.3.5.2.zip\nCollecting lazy (from ibmdbpy)\n  Downloading lazy-1.3.zip\nCollecting pandas (from ibmdbpy)\n  Downloading pandas-0.21.0-cp35-cp35m-manylinux1_x86_64.whl (25.7MB)\n\u001b[K    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 25.7MB 46.7MB/s ta 0:00:01   36% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b                    | 9.3MB 50.3MB/s eta 0:00:01\n\u001b[?25hCollecting setuptools (from pypyodbc->ibmdbpy)\n  Downloading setuptools-36.6.0-py2.py3-none-any.whl (481kB)\n\u001b[K    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 481kB 55.7MB/s ta 0:00:01\n\u001b[?25hCollecting python-dateutil>=2 (from pandas->ibmdbpy)\n  Downloading python_dateutil-2.6.1-py2.py3-none-any.whl (194kB)\n\u001b[K    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 194kB 60.6MB/s ta 0:00:01\n\u001b[?25hCollecting pytz>=2011k (from pandas->ibmdbpy)\n  Downloading pytz-2017.3-py2.py3-none-any.whl (511kB)\n\u001b[K    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 512kB 49.0MB/s ta 0:00:01\n\u001b[?25hBuilding wheels for collected packages: future, pypyodbc, lazy\n  Running setup.py bdist_wheel for future ... \u001b[?25ldone\n\u001b[?25h  Stored in directory: /gpfs/fs01/user/sd78-6abf89da5147f6-d87d304b3991/.cache/pip/wheels/c2/50/7c/0d83b4baac4f63ff7a765bd16390d2ab43c93587fac9d6017a\n  Running setup.py bdist_wheel for pypyodbc ... \u001b[?25ldone\n\u001b[?25h  Stored in directory: /gpfs/fs01/user/sd78-6abf89da5147f6-d87d304b3991/.cache/pip/wheels/61/d0/8b/bbe588c1a19ff4ca4fbbe8d70c58e5d38d9302b92a60bef58a\n  Running setup.py bdist_wheel for lazy ... \u001b[?25ldone\n\u001b[?25h  Stored in directory: /gpfs/fs01/user/sd78-6abf89da5147f6-d87d304b3991/.cache/pip/wheels/7a/28/57/4973c5363d56c74a4d4c4eff6080287d861305a10eb08b7b8f\nSuccessfully built future pypyodbc lazy\nInstalling collected packages: six, future, numpy, setuptools, pypyodbc, lazy, python-dateutil, pytz, pandas\nSuccessfully installed future-0.16.0 lazy-1.3 numpy-1.13.3 pandas-0.21.0 pypyodbc-1.3.4 python-dateutil-2.6.1 pytz-2017.3 setuptools-36.6.0 six-1.11.0\nCollecting pixiedust\n  Downloading pixiedust-1.1.1.tar.gz (168kB)\n\u001b[K    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 174kB 38.4MB/s ta 0:00:01\n\u001b[?25hCollecting mpld3 (from pixiedust)\n  Downloading mpld3-0.3.tar.gz (788kB)\n\u001b[K    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 798kB 18.7MB/s ta 0:00:01\n\u001b[?25hCollecting lxml (from pixiedust)\n  Downloading lxml-4.1.0-cp35-cp35m-manylinux1_x86_64.whl (5.5MB)\n\u001b[K    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5.6MB 51.6MB/s ta 0:00:01\n\u001b[?25hCollecting geojson (from pixiedust)\n  Downloading geojson-2.3.0-py2.py3-none-any.whl\nRequirement already up-to-date: jupyter_kernel_gateway in /usr/local/src/conda3_runtime.v20/4.1.1/lib/python3.5/site-packages (from pixiedust)\nCollecting astunparse (from pixiedust)\n  Downloading astunparse-1.5.0-py2.py3-none-any.whl\nCollecting tornado>=4.2.0 (from jupyter_kernel_gateway->pixiedust)\n  Downloading tornado-4.5.2.tar.gz (483kB)\n\u001b[K    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 491kB 52.1MB/s ta 0:00:01    67% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a          | 327kB 47.9MB/s eta 0:00:01\n\u001b[?25hCollecting traitlets>=4.2.0 (from jupyter_kernel_gateway->pixiedust)\n  Downloading traitlets-4.3.2-py2.py3-none-any.whl (74kB)\n\u001b[K    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 81kB 37.3MB/s ta 0:00:01\n\u001b[?25hCollecting jupyter-core>=4.0 (from jupyter_kernel_gateway->pixiedust)\n  Downloading jupyter_core-4.4.0-py2.py3-none-any.whl (126kB)\n\u001b[K    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 133kB 55.1MB/s ta 0:00:01\n\u001b[?25hCollecting requests<3.0,>=2.7 (from jupyter_kernel_gateway->pixiedust)\n  Downloading requests-2.18.4-py2.py3-none-any.whl (88kB)\n\u001b[K    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 92kB 56.6MB/s ta 0:00:01\n\u001b[?25hCollecting jupyter-client>=4.2.0 (from jupyter_kernel_gateway->pixiedust)\n  Downloading jupyter_client-5.1.0-py2.py3-none-any.whl (84kB)\n\u001b[K    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 92kB 57.0MB/s ta 0:00:01\n\u001b[?25hCollecting notebook<6.0,>=5.0.0 (from jupyter_kernel_gateway->pixiedust)\n  Downloading notebook-5.2.0-py2.py3-none-any.whl (8.0MB)\n\u001b[K    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 8.0MB 52.7MB/s ta 0:00:01\n\u001b[?25hCollecting wheel<1.0,>=0.23.0 (from astunparse->pixiedust)\n  Downloading wheel-0.30.0-py2.py3-none-any.whl (49kB)\n\u001b[K    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51kB 58.1MB/s ta 0:00:01\n\u001b[?25hRequirement already up-to-date: six<2.0,>=1.6.1 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/sd78-6abf89da5147f6-d87d304b3991/.local/lib/python3.5/site-packages (from astunparse->pixiedust)\nCollecting decorator (from traitlets>=4.2.0->jupyter_kernel_gateway->pixiedust)\n  Downloading decorator-4.1.2-py2.py3-none-any.whl\nCollecting ipython-genutils (from traitlets>=4.2.0->jupyter_kernel_gateway->pixiedust)\n  Downloading ipython_genutils-0.2.0-py2.py3-none-any.whl\nCollecting urllib3<1.23,>=1.21.1 (from requests<3.0,>=2.7->jupyter_kernel_gateway->pixiedust)\n  Downloading urllib3-1.22-py2.py3-none-any.whl (132kB)\n\u001b[K    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 133kB 61.1MB/s ta 0:00:01\n\u001b[?25hCollecting chardet<3.1.0,>=3.0.2 (from requests<3.0,>=2.7->jupyter_kernel_gateway->pixiedust)\n  Downloading chardet-3.0.4-py2.py3-none-any.whl (133kB)\n\u001b[K    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 143kB 37.8MB/s ta 0:00:01\n\u001b[?25hRequirement already up-to-date: certifi>=2017.4.17 in /usr/local/src/conda3_runtime.v20/4.1.1/lib/python3.5/site-packages (from requests<3.0,>=2.7->jupyter_kernel_gateway->pixiedust)\nCollecting idna<2.7,>=2.5 (from requests<3.0,>=2.7->jupyter_kernel_gateway->pixiedust)\n  Downloading idna-2.6-py2.py3-none-any.whl (56kB)\n\u001b[K    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 61kB 14.2MB/s ta 0:00:01\n\u001b[?25hRequirement already up-to-date: python-dateutil>=2.1 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/sd78-6abf89da5147f6-d87d304b3991/.local/lib/python3.5/site-packages (from jupyter-client>=4.2.0->jupyter_kernel_gateway->pixiedust)\nCollecting pyzmq>=13 (from jupyter-client>=4.2.0->jupyter_kernel_gateway->pixiedust)\n  Downloading pyzmq-16.0.3-cp35-cp35m-manylinux1_x86_64.whl (3.1MB)\n\u001b[K    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3.1MB 36.0MB/s ta 0:00:01    80% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589      | 2.5MB 46.5MB/s eta 0:00:01\n\u001b[?25hCollecting ipykernel (from notebook<6.0,>=5.0.0->jupyter_kernel_gateway->pixiedust)\n  Downloading ipykernel-4.6.1-py3-none-any.whl (104kB)\n\u001b[K    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 112kB 40.9MB/s ta 0:00:01\n\u001b[?25hCollecting jinja2 (from notebook<6.0,>=5.0.0->jupyter_kernel_gateway->pixiedust)\n  Downloading Jinja2-2.9.6-py2.py3-none-any.whl (340kB)\n\u001b[K    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 348kB 18.7MB/s ta 0:00:01\n\u001b[?25hRequirement already up-to-date: terminado>=0.3.3; sys_platform != \"win32\" in /usr/local/src/conda3_runtime.v20/4.1.1/lib/python3.5/site-packages (from notebook<6.0,>=5.0.0->jupyter_kernel_gateway->pixiedust)\nCollecting nbformat (from notebook<6.0,>=5.0.0->jupyter_kernel_gateway->pixiedust)\n  Downloading nbformat-4.4.0-py2.py3-none-any.whl (155kB)\n\u001b[K    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 163kB 38.4MB/s ta 0:00:01\n\u001b[?25hCollecting nbconvert (from notebook<6.0,>=5.0.0->jupyter_kernel_gateway->pixiedust)\n  Downloading nbconvert-5.3.1-py2.py3-none-any.whl (387kB)\n\u001b[K    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 389kB 48.2MB/s ta 0:00:01\n\u001b[?25hCollecting ipython>=4.0.0 (from ipykernel->notebook<6.0,>=5.0.0->jupyter_kernel_gateway->pixiedust)\n  Downloading ipython-6.2.1-py3-none-any.whl (745kB)\n\u001b[K    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 747kB 32.0MB/s ta 0:00:01\n\u001b[?25hCollecting MarkupSafe>=0.23 (from jinja2->notebook<6.0,>=5.0.0->jupyter_kernel_gateway->pixiedust)\n  Downloading MarkupSafe-1.0.tar.gz\nCollecting jsonschema!=2.5.0,>=2.4 (from nbformat->notebook<6.0,>=5.0.0->jupyter_kernel_gateway->pixiedust)\n  Downloading jsonschema-2.6.0-py2.py3-none-any.whl\nCollecting testpath (from nbconvert->notebook<6.0,>=5.0.0->jupyter_kernel_gateway->pixiedust)\n  Downloading testpath-0.3.1-py2.py3-none-any.whl (161kB)\n\u001b[K    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 163kB 41.0MB/s ta 0:00:01\n\u001b[?25hCollecting entrypoints>=0.2.2 (from nbconvert->notebook<6.0,>=5.0.0->jupyter_kernel_gateway->pixiedust)\n  Downloading entrypoints-0.2.3-py2.py3-none-any.whl\nCollecting pandocfilters>=1.4.1 (from nbconvert->notebook<6.0,>=5.0.0->jupyter_kernel_gateway->pixiedust)\n  Downloading pandocfilters-1.4.2.tar.gz\nCollecting bleach (from nbconvert->notebook<6.0,>=5.0.0->jupyter_kernel_gateway->pixiedust)\n  Downloading bleach-2.1.1-py2.py3-none-any.whl\nCollecting mistune>=0.7.4 (from nbconvert->notebook<6.0,>=5.0.0->jupyter_kernel_gateway->pixiedust)\n  Downloading mistune-0.8-py2.py3-none-any.whl\nCollecting pygments (from nbconvert->notebook<6.0,>=5.0.0->jupyter_kernel_gateway->pixiedust)\n  Downloading Pygments-2.2.0-py2.py3-none-any.whl (841kB)\n\u001b[K    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 849kB 26.5MB/s ta 0:00:01\n\u001b[?25hCollecting jedi>=0.10 (from ipython>=4.0.0->ipykernel->notebook<6.0,>=5.0.0->jupyter_kernel_gateway->pixiedust)\n  Downloading jedi-0.11.0-py2.py3-none-any.whl (146kB)\n\u001b[K    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 153kB 34.4MB/s ta 0:00:01\n\u001b[?25hRequirement already up-to-date: simplegeneric>0.8 in /usr/local/src/conda3_runtime.v20/4.1.1/lib/python3.5/site-packages (from ipython>=4.0.0->ipykernel->notebook<6.0,>=5.0.0->jupyter_kernel_gateway->pixiedust)\nRequirement already up-to-date: setuptools>=18.5 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/sd78-6abf89da5147f6-d87d304b3991/.local/lib/python3.5/site-packages (from ipython>=4.0.0->ipykernel->notebook<6.0,>=5.0.0->jupyter_kernel_gateway->pixiedust)\nCollecting pickleshare (from ipython>=4.0.0->ipykernel->notebook<6.0,>=5.0.0->jupyter_kernel_gateway->pixiedust)\n  Downloading pickleshare-0.7.4-py2.py3-none-any.whl\nCollecting pexpect; sys_platform != \"win32\" (from ipython>=4.0.0->ipykernel->notebook<6.0,>=5.0.0->jupyter_kernel_gateway->pixiedust)\n  Downloading pexpect-4.2.1-py2.py3-none-any.whl (55kB)\n\u001b[K    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 61kB 24.7MB/s ta 0:00:01\n\u001b[?25hCollecting prompt-toolkit<2.0.0,>=1.0.4 (from ipython>=4.0.0->ipykernel->notebook<6.0,>=5.0.0->jupyter_kernel_gateway->pixiedust)\n  Downloading prompt_toolkit-1.0.15-py3-none-any.whl (247kB)\n\u001b[K    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 256kB 19.7MB/s ta 0:00:01\n\u001b[?25hCollecting html5lib!=1.0b1,!=1.0b2,!=1.0b3,!=1.0b4,!=1.0b5,!=1.0b6,!=1.0b7,!=1.0b8,>=0.99999999pre (from bleach->nbconvert->notebook<6.0,>=5.0.0->jupyter_kernel_gateway->pixiedust)\n  Downloading html5lib-1.0b10-py2.py3-none-any.whl (112kB)\n\u001b[K    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 112kB 17.8MB/s ta 0:00:01\n\u001b[?25hCollecting parso==0.1.0 (from jedi>=0.10->ipython>=4.0.0->ipykernel->notebook<6.0,>=5.0.0->jupyter_kernel_gateway->pixiedust)\n  Downloading parso-0.1.0-py2.py3-none-any.whl (89kB)\n\u001b[K    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 92kB 28.9MB/s ta 0:00:01\n\u001b[?25hCollecting ptyprocess>=0.5 (from pexpect; sys_platform != \"win32\"->ipython>=4.0.0->ipykernel->notebook<6.0,>=5.0.0->jupyter_kernel_gateway->pixiedust)\n  Downloading ptyprocess-0.5.2-py2.py3-none-any.whl\nCollecting wcwidth (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipykernel->notebook<6.0,>=5.0.0->jupyter_kernel_gateway->pixiedust)\n  Downloading wcwidth-0.1.7-py2.py3-none-any.whl\nRequirement already up-to-date: webencodings in /usr/local/src/conda3_runtime.v20/4.1.1/lib/python3.5/site-packages (from html5lib!=1.0b1,!=1.0b2,!=1.0b3,!=1.0b4,!=1.0b5,!=1.0b6,!=1.0b7,!=1.0b8,>=0.99999999pre->bleach->nbconvert->notebook<6.0,>=5.0.0->jupyter_kernel_gateway->pixiedust)\nBuilding wheels for collected packages: pixiedust, mpld3, tornado, MarkupSafe, pandocfilters\n  Running setup.py bdist_wheel for pixiedust ... \u001b[?25ldone\n\u001b[?25h  Stored in directory: /gpfs/fs01/user/sd78-6abf89da5147f6-d87d304b3991/.cache/pip/wheels/b1/3a/91/bd59025e8c17f7a41e230948bcdbb95dd17606cf9db053ff7f\n  Running setup.py bdist_wheel for mpld3 ... \u001b[?25ldone\n\u001b[?25h  Stored in directory: /gpfs/fs01/user/sd78-6abf89da5147f6-d87d304b3991/.cache/pip/wheels/69/bc/68/7ca3b696749d183e998968fc24b0ff3c5e119d9e68bf495b07\n  Running setup.py bdist_wheel for tornado ... \u001b[?25ldone\n\u001b[?25h  Stored in directory: /gpfs/fs01/user/sd78-6abf89da5147f6-d87d304b3991/.cache/pip/wheels/b3/47/3a/96e12476829cb196adabc879fedb72f1bb2c8613b6961e78e7\n  Running setup.py bdist_wheel for MarkupSafe ... \u001b[?25ldone\n\u001b[?25h  Stored in directory: /gpfs/fs01/user/sd78-6abf89da5147f6-d87d304b3991/.cache/pip/wheels/88/a7/30/e39a54a87bcbe25308fa3ca64e8ddc75d9b3e5afa21ee32d57\n  Running setup.py bdist_wheel for pandocfilters ... \u001b[?25ldone\n\u001b[?25h  Stored in directory: /gpfs/fs01/user/sd78-6abf89da5147f6-d87d304b3991/.cache/pip/wheels/08/5b/5b/66b3cde6f8215f8345479ef3699d6ddbb860f6ea7072008f8b\nSuccessfully built pixiedust mpld3 tornado MarkupSafe pandocfilters\nInstalling collected packages: mpld3, lxml, geojson, wheel, astunparse, pixiedust, tornado, decorator, ipython-genutils, traitlets, jupyter-core, urllib3, chardet, idna, requests, pyzmq, jupyter-client, parso, jedi, pickleshare, ptyprocess, pexpect, wcwidth, prompt-toolkit, pygments, ipython, ipykernel, MarkupSafe, jinja2, jsonschema, nbformat, testpath, entrypoints, pandocfilters, html5lib, bleach, mistune, nbconvert, notebook\nSuccessfully installed MarkupSafe-1.0 astunparse-1.5.0 bleach-2.1.1 chardet-3.0.4 decorator-4.1.2 entrypoints-0.2.3 geojson-2.3.0 html5lib-1.0b10 idna-2.6 ipykernel-4.6.1 ipython-6.2.1 ipython-genutils-0.2.0 jedi-0.11.0 jinja2-2.9.6 jsonschema-2.6.0 jupyter-client-5.1.0 jupyter-core-4.4.0 lxml-4.1.0 mistune-0.8 mpld3-0.3 nbconvert-5.3.1 nbformat-4.4.0 notebook-5.2.0 pandocfilters-1.4.2 parso-0.1.0 pexpect-4.2.1 pickleshare-0.7.4 pixiedust-1.1.1 prompt-toolkit-1.0.15 ptyprocess-0.5.2 pygments-2.2.0 pyzmq-16.0.3 requests-2.18.4 testpath-0.3.1 tornado-4.5.2 traitlets-4.3.2 urllib3-1.22 wcwidth-0.1.7 wheel-0.30.0\n"
                }
            ], 
            "cell_type": "code"
        }, 
        {
            "source": "## Import the required libraries", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "# Imports for DB2 Warehouse\nimport jaydebeapi\nfrom ibmdbpy import IdaDataBase\nfrom ibmdbpy import IdaDataFrame\n\n#Imports for Spark\nfrom pyspark.ml.feature import StringIndexer, IndexToString\nfrom pyspark.ml.feature import Bucketizer\nfrom pyspark.mllib.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import Normalizer\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator\nfrom pyspark.ml.classification import NaiveBayes, DecisionTreeClassifier\nfrom pyspark.sql.functions import year\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n\n# Imports for pixiedust\nfrom pixiedust.display import *", 
            "metadata": {}, 
            "execution_count": 4, 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Pixiedust database opened successfully\n"
                }, 
                {
                    "data": {
                        "text/plain": "<IPython.core.display.HTML object>", 
                        "text/html": "\n        <div style=\"margin:10px\">\n            <a href=\"https://github.com/ibm-watson-data-lab/pixiedust\" target=\"_new\">\n                <img src=\"https://github.com/ibm-watson-data-lab/pixiedust/raw/master/docs/_static/pd_icon32.png\" style=\"float:left;margin-right:10px\"/>\n            </a>\n            <span>Pixiedust version 1.1.1</span>\n        </div>\n        "
                    }, 
                    "metadata": {}, 
                    "output_type": "display_data"
                }
            ], 
            "cell_type": "code"
        }, 
        {
            "source": "<a id=\"database\"></a>\n## Connect to the database and read in our data\n\nSelect the <b>Find and Add Data</b> icon <br>\n<img alt=\"IBM Bluemix.Get started now\" src=\"https://raw.githubusercontent.com/jpatter/LMCO/master/Lab-1/images/connections-button.png\" >\n\nSelect the <b>Connections</b> view and then <b>Insert to code</b>.\n\n<img alt=\"IBM Bluemix.Get started now\" src=\"https://raw.githubusercontent.com/jpatter/LMCO/master/Lab-1/images/InsertToCode.PNG\" >\n\nSelect <b>Insert SparkSession DataFrame</b> and select the schema (will start with DASH but will likely NOT be the same value you see in the image) and table (should only be one). Then select <b>Insert Code</b>.\n\n<img alt=\"IBM Bluemix.Get started now\" src=\"https://raw.githubusercontent.com/jpatter/LMCO/master/Lab-1/images/InsertCode.PNG\" ><br>\nRename the result to <b>trafficking_df</b> to ensure compliance with the following cells.\n\n", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "# The code was removed by DSX for sharing.", 
            "metadata": {}, 
            "execution_count": 5, 
            "outputs": [
                {
                    "execution_count": 5, 
                    "data": {
                        "text/plain": "Row(INTERNAL_ID=363, VETTING_LEVEL=100, DESCRIPTION='NA', VETTING_LEVEL_PREDICTION=30, ALGORITHM_NAME='DECISION_TREE', NAME='Joyce Valerie Jones', GENDER='F', BIRTH_DATE=datetime.date(1990, 11, 25), BIRTH_COUNTRY='Ghana', BIRTH_COUNTRY_CODE='GH', OCCUPATION='Oncologist', ADDRESS='934 Connie Burgs, Three Rivers, Michigan 49093', SSN='065-42-3213', PASSPORT_NUMBER=257734367, PASSPORT_COUNTRY='Ghana', PASSPORT_COUNTRY_CODE='GH', COUNTRIES_VISITED='FI,UZ', COUNTRIES_VISITED_COUNT=2, ARRIVAL_AIRPORT_COUNTRY_CODE='US', ARRIVAL_AIRPORT_IATA='SAT', ARRIVAL_AIRPORT_MUNICIPALITY='San Antonio', ARRIVAL_AIRPORT_REGION='US-TX', DEPARTURE_AIRPORT_COUNTRY_CODE='FI', DEPARTURE_AIRPORT_IATA='HEL', DEPARTURE_AIRPORT_MUNICIPALITY='Helsinki', DEPARTURE_AIRPORT_REGION='FI-ES', UUID='13363a0f-1a5e-4f68-9e53-a9d52845f33b', AGE=26)"
                    }, 
                    "metadata": {}, 
                    "output_type": "execute_result"
                }
            ], 
            "cell_type": "code"
        }, 
        {
            "source": "<a id=\"transform\"></a>\n## Identify our labels and transform \n\nWe will use the 'VETTING_LEVEL' column as a label for training the machine learning model.  This is where our analyst has marked the data as vetted.  \n\nSpark ML requires that that the labels are data type Double, so we will cast the  column as Double (it was inferred as Integer when read into Spark).\n\nwithColumn() is a Spark SQL way to manipulate a dataframe.  Since an RDD is immutable, we create a new RDD each time we transform.  This code creates a new column VettingTemp and sets it to the values in \"VETTING_LEVEL\" cast to a Double.    It then drops column VETTING_LEVEL and renames column VettingTemp to VETTING_LEVEL.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "DataWithLabels = (trafficking_df.withColumn(\"VettingTemp\", trafficking_df[\"VETTING_LEVEL\"]\n    .cast(\"Double\")).drop(\"VETTING_LEVEL\").withColumnRenamed(\"VettingTemp\", \"VETTING_LEVEL\"))", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 6, 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "We want to use year of birth intead of date of birth in our learning.  \n\nAnother way to transform an rdd in Spark is using SQL Syntax.  Here, we will be adding a new field, BIRTH_YEAR to our vetting set.  We will also just select the fields we need.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "DataWithLabels.createOrReplaceTempView(\"VettingData\")\nAllVettingData = sqlContext.sql (\"SELECT UUID, VETTING_LEVEL, NAME, OCCUPATION, COUNTRIES_VISITED_COUNT, PASSPORT_COUNTRY_CODE, GENDER, year(BIRTH_DATE) as BIRTH_YEAR, 1 as Counter FROM VettingData\")\nFilteredVettingData = AllVettingData.filter(\"VETTING_LEVEL==100\")\n\nFilteredVettingData.count()", 
            "metadata": {}, 
            "execution_count": 7, 
            "outputs": [
                {
                    "execution_count": 7, 
                    "data": {
                        "text/plain": "907"
                    }, 
                    "metadata": {}, 
                    "output_type": "execute_result"
                }
            ], 
            "cell_type": "code"
        }, 
        {
            "source": "Use pixiedust to visually explore the data.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "display(AllVettingData)", 
            "metadata": {
                "pixiedust": {
                    "displayParams": {
                        "keyFields": "VETTING_LEVEL", 
                        "title": "Vetting level histogram", 
                        "aggregation": "SUM", 
                        "valueFields": "Counter", 
                        "handlerId": "barChart", 
                        "rowCount": "100", 
                        "rendererId": "bokeh"
                    }
                }
            }, 
            "execution_count": 8, 
            "outputs": [
                {
                    "data": {
                        "text/plain": "<IPython.core.display.HTML object>", 
                        "text/html": "<style type=\"text/css\">.pd_warning{display:none;}</style><div class=\"pd_warning\"><em>Hey, there's something awesome here! To see it, open this notebook outside GitHub, in a viewer like Jupyter</em></div>\n        <div class=\"pd_save is-viewer-good\" style=\"padding-right:10px;text-align: center;line-height:initial !important;font-size: xx-large;font-weight: 500;color: coral;\">\n            Vetting level histogram\n        </div>\n    \n        <div id=\"chartFigure88713d18\" class=\"pd_save\" style=\"overflow-x:auto\">\n            \n                    <script class=\"pd_save\">\n                    if ( !window.Bokeh && !window.autoload){\n                        window.autoload=true;\n                        \n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    if (root.Bokeh !== undefined) {\n      var el = document.getElementById(\"\");\n      if (el != null) {\n        el.textContent = \"BokehJS \" + Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n    }\n    finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.info(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(js_urls, callback) {\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = js_urls.length;\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var s = document.createElement('script');\n      s.src = url;\n      s.async = false;\n      s.onreadystatechange = s.onload = function() {\n        root._bokeh_is_loading--;\n        if (root._bokeh_is_loading === 0) {\n          console.log(\"Bokeh: all BokehJS libraries loaded\");\n          run_callbacks()\n        }\n      };\n      s.onerror = function() {\n        console.warn(\"failed to load library \" + url);\n      };\n      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.getElementsByTagName(\"head\")[0].appendChild(s);\n    }\n  };\n\n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.7.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.7.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.7.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-0.12.7.min.js\"];\n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    \n    function(Bokeh) {\n      \n    },\n    function(Bokeh) {\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.7.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.7.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.7.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.7.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.7.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.7.min.css\");\n    }\n  ];\n\n  function run_inline_js() {\n    \n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(js_urls, function() {\n      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));  \n                    }\n                    </script>\n                    \n\n    <div class=\"bk-root\">\n        <div class=\"bk-plotdiv\" id=\"af38d5b2-adff-44a4-a201-62e73dde7858\"></div>\n    </div>\n<script type=\"text/javascript\">\n  \n  (function(root) {\n    function now() {\n      return new Date();\n    }\n  \n    var force = false;\n  \n    if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n      root._bokeh_onload_callbacks = [];\n      root._bokeh_is_loading = undefined;\n    }\n  \n  \n    \n    if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n      root._bokeh_timeout = Date.now() + 0;\n      root._bokeh_failed_load = false;\n    }\n  \n    var NB_LOAD_WARNING = {'data': {'text/html':\n       \"<div style='background-color: #fdd'>\\n\"+\n       \"<p>\\n\"+\n       \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n       \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n       \"</p>\\n\"+\n       \"<ul>\\n\"+\n       \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n       \"<li>use INLINE resources instead, as so:</li>\\n\"+\n       \"</ul>\\n\"+\n       \"<code>\\n\"+\n       \"from bokeh.resources import INLINE\\n\"+\n       \"output_notebook(resources=INLINE)\\n\"+\n       \"</code>\\n\"+\n       \"</div>\"}};\n  \n    function display_loaded() {\n      if (root.Bokeh !== undefined) {\n        var el = document.getElementById(\"af38d5b2-adff-44a4-a201-62e73dde7858\");\n        if (el != null) {\n          el.textContent = \"BokehJS \" + Bokeh.version + \" successfully loaded.\";\n        }\n      } else if (Date.now() < root._bokeh_timeout) {\n        setTimeout(display_loaded, 100)\n      }\n    }\n  \n  \n    function run_callbacks() {\n      try {\n        root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n      }\n      finally {\n        delete root._bokeh_onload_callbacks\n      }\n      console.info(\"Bokeh: all callbacks have finished\");\n    }\n  \n    function load_libs(js_urls, callback) {\n      root._bokeh_onload_callbacks.push(callback);\n      if (root._bokeh_is_loading > 0) {\n        console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n        return null;\n      }\n      if (js_urls == null || js_urls.length === 0) {\n        run_callbacks();\n        return null;\n      }\n      console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n      root._bokeh_is_loading = js_urls.length;\n      for (var i = 0; i < js_urls.length; i++) {\n        var url = js_urls[i];\n        var s = document.createElement('script');\n        s.src = url;\n        s.async = false;\n        s.onreadystatechange = s.onload = function() {\n          root._bokeh_is_loading--;\n          if (root._bokeh_is_loading === 0) {\n            console.log(\"Bokeh: all BokehJS libraries loaded\");\n            run_callbacks()\n          }\n        };\n        s.onerror = function() {\n          console.warn(\"failed to load library \" + url);\n        };\n        console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n        document.getElementsByTagName(\"head\")[0].appendChild(s);\n      }\n    };var element = document.getElementById(\"af38d5b2-adff-44a4-a201-62e73dde7858\");\n    if (element == null) {\n      console.log(\"Bokeh: ERROR: autoload.js configured with elementid 'af38d5b2-adff-44a4-a201-62e73dde7858' but no matching script tag was found. \")\n      return false;\n    }\n  \n    var js_urls = [];\n  \n    var inline_js = [\n      function(Bokeh) {\n        (function() {\n          var fn = function() {\n            var docs_json = {\"98e53efe-c5ef-4691-8e3e-cff3e18c7e42\":{\"roots\":{\"references\":[{\"attributes\":{\"bottom_units\":\"screen\",\"fill_alpha\":{\"value\":0.5},\"fill_color\":{\"value\":\"lightgrey\"},\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":{\"value\":1.0},\"line_color\":{\"value\":\"black\"},\"line_dash\":[4,4],\"line_width\":{\"value\":2},\"plot\":null,\"render_mode\":\"css\",\"right_units\":\"screen\",\"top_units\":\"screen\"},\"id\":\"c6dee363-399b-41e8-b340-b214f1aa0ebc\",\"type\":\"BoxAnnotation\"},{\"attributes\":{},\"id\":\"14128142-b829-4587-a96f-2cce08196d4f\",\"type\":\"CategoricalTicker\"},{\"attributes\":{\"label\":{\"value\":\"30.0\"},\"renderers\":[{\"id\":\"21ac3e57-2bfa-4f35-895c-9e28e85f29e1\",\"type\":\"GlyphRenderer\"}]},\"id\":\"5b859b47-1d3e-43a0-aba3-f4b1cd48e9fd\",\"type\":\"LegendItem\"},{\"attributes\":{\"callback\":null,\"column_names\":[\"width\",\"line_color\",\"x\",\"fill_alpha\",\"line_alpha\",\"y\",\"label\",\"height\",\"color\"],\"data\":{\"VETTING_LEVEL\":[100.0],\"chart_index\":[{\"VETTING_LEVEL\":100.0}],\"color\":[\"#f22c40\"],\"fill_alpha\":[0.8],\"height\":[907.0],\"label\":[{\"VETTING_LEVEL\":100.0}],\"line_alpha\":[1.0],\"line_color\":[\"white\"],\"width\":[0.8],\"x\":[\"100.0\"],\"y\":[453.5]}},\"id\":\"87665fa8-9e63-43ee-b2d6-203693bf76ad\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"callback\":null,\"column_names\":[\"width\",\"line_color\",\"x\",\"fill_alpha\",\"line_alpha\",\"y\",\"label\",\"height\",\"color\"],\"data\":{\"VETTING_LEVEL\":[10.0],\"chart_index\":[{\"VETTING_LEVEL\":10.0}],\"color\":[\"#f22c40\"],\"fill_alpha\":[0.8],\"height\":[42.0],\"label\":[{\"VETTING_LEVEL\":10.0}],\"line_alpha\":[1.0],\"line_color\":[\"white\"],\"width\":[0.8],\"x\":[\"10.0\"],\"y\":[21.0]}},\"id\":\"0e30cc19-69aa-416d-b5b4-abd05f461449\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"callback\":null,\"end\":952.35},\"id\":\"8c9127d0-eca3-4918-aee9-f81296f01b93\",\"type\":\"Range1d\"},{\"attributes\":{},\"id\":\"1954f13f-0050-47d2-8409-4b79614b35dd\",\"type\":\"SaveTool\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_scroll\":\"auto\",\"active_tap\":\"auto\",\"tools\":[{\"id\":\"8631a84e-9993-4d23-bc6e-d1f0f6fdafaa\",\"type\":\"PanTool\"},{\"id\":\"027f09d6-20bf-404f-9f36-606f08d11795\",\"type\":\"WheelZoomTool\"},{\"id\":\"da6b387c-c612-46cf-95d0-9c9b194f9961\",\"type\":\"BoxZoomTool\"},{\"id\":\"1954f13f-0050-47d2-8409-4b79614b35dd\",\"type\":\"SaveTool\"},{\"id\":\"c713f4d0-5c50-44e8-940f-4cbc447be1a6\",\"type\":\"ResetTool\"},{\"id\":\"3060d8b7-1b9b-44e8-b619-91c7ae046de9\",\"type\":\"HelpTool\"}]},\"id\":\"58ae3922-402e-4f0f-8694-d8517891dde0\",\"type\":\"Toolbar\"},{\"attributes\":{\"children\":[{\"id\":\"c12c2848-e17a-4a32-9f71-e047691dc516\",\"type\":\"Row\"}]},\"id\":\"fa729e72-a60e-4e99-9342-890be1e6daad\",\"type\":\"Column\"},{\"attributes\":{\"fill_alpha\":{\"field\":\"fill_alpha\"},\"fill_color\":{\"field\":\"color\"},\"height\":{\"field\":\"height\",\"units\":\"data\"},\"line_color\":{\"field\":\"line_color\"},\"width\":{\"field\":\"width\",\"units\":\"data\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"8c968091-ff11-46c2-a1ba-7e9179d55733\",\"type\":\"Rect\"},{\"attributes\":{},\"id\":\"3060d8b7-1b9b-44e8-b619-91c7ae046de9\",\"type\":\"HelpTool\"},{\"attributes\":{\"plot\":null,\"text\":null},\"id\":\"525a758d-e93a-4b20-a3df-6d14ab543998\",\"type\":\"Title\"},{\"attributes\":{\"children\":[{\"id\":\"ddf1f9f3-8355-4d93-bc89-8558f165b5a0\",\"type\":\"ToolbarBox\"},{\"id\":\"fa729e72-a60e-4e99-9342-890be1e6daad\",\"type\":\"Column\"}]},\"id\":\"1718efda-ad76-4779-bfef-c6a4421900cd\",\"type\":\"Column\"},{\"attributes\":{\"dimension\":1,\"plot\":{\"id\":\"a07b2e40-f666-479e-9207-8f62beafda2e\",\"subtype\":\"Chart\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"3b5f21c3-55b9-46d6-8071-283e23f1d924\",\"type\":\"BasicTicker\"}},\"id\":\"42922542-9c33-493b-8c0b-56451d0adcfb\",\"type\":\"Grid\"},{\"attributes\":{\"items\":[{\"id\":\"f6abe2d7-5730-47e8-bfd3-c8ad1e5b18b0\",\"type\":\"LegendItem\"},{\"id\":\"69109cb2-0e54-4261-9fa1-91a228a680f9\",\"type\":\"LegendItem\"},{\"id\":\"5b859b47-1d3e-43a0-aba3-f4b1cd48e9fd\",\"type\":\"LegendItem\"},{\"id\":\"9cee4cf6-2116-49b8-a4b9-2e45516c27ad\",\"type\":\"LegendItem\"}],\"location\":\"top_left\",\"plot\":{\"id\":\"a07b2e40-f666-479e-9207-8f62beafda2e\",\"subtype\":\"Chart\",\"type\":\"Plot\"}},\"id\":\"cdccf352-5a37-4311-8669-e7af7bb62f7e\",\"type\":\"Legend\"},{\"attributes\":{\"source\":null},\"id\":\"dd5e39d9-f841-4b27-951c-a6eb747be248\",\"type\":\"CDSView\"},{\"attributes\":{},\"id\":\"027f09d6-20bf-404f-9f36-606f08d11795\",\"type\":\"WheelZoomTool\"},{\"attributes\":{\"data_source\":{\"id\":\"87665fa8-9e63-43ee-b2d6-203693bf76ad\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"1876cc38-adf5-42c5-ac4d-c40fc6092ee9\",\"type\":\"Rect\"},\"hover_glyph\":null,\"muted_glyph\":null,\"view\":{\"id\":\"5df08288-42ee-4cff-8e03-4117e8dd64e3\",\"type\":\"CDSView\"}},\"id\":\"9d07c455-8b69-4c9e-bc3d-fa789070358d\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"label\":{\"value\":\"10.0\"},\"renderers\":[{\"id\":\"04728b7e-cb50-49ce-a79a-510959682a94\",\"type\":\"GlyphRenderer\"}]},\"id\":\"f6abe2d7-5730-47e8-bfd3-c8ad1e5b18b0\",\"type\":\"LegendItem\"},{\"attributes\":{},\"id\":\"859fb55e-6ce7-422d-8ad8-d5a02af1fa99\",\"type\":\"LinearScale\"},{\"attributes\":{},\"id\":\"8631a84e-9993-4d23-bc6e-d1f0f6fdafaa\",\"type\":\"PanTool\"},{\"attributes\":{\"data_source\":{\"id\":\"0e30cc19-69aa-416d-b5b4-abd05f461449\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"4e9c0b08-3dbc-4b5c-bc68-b1f37e02e8e5\",\"type\":\"Rect\"},\"hover_glyph\":null,\"muted_glyph\":null,\"view\":{\"id\":\"d0d2051d-61b8-431a-9836-ed3583e88c6c\",\"type\":\"CDSView\"}},\"id\":\"04728b7e-cb50-49ce-a79a-510959682a94\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"callback\":null,\"factors\":[\"10.0\",\"20.0\",\"30.0\",\"100.0\"]},\"id\":\"907fd325-aed3-4f0f-8ca0-84fb99e86694\",\"type\":\"FactorRange\"},{\"attributes\":{\"callback\":null,\"column_names\":[\"width\",\"line_color\",\"x\",\"fill_alpha\",\"line_alpha\",\"y\",\"label\",\"height\",\"color\"],\"data\":{\"VETTING_LEVEL\":[30.0],\"chart_index\":[{\"VETTING_LEVEL\":30.0}],\"color\":[\"#f22c40\"],\"fill_alpha\":[0.8],\"height\":[96.0],\"label\":[{\"VETTING_LEVEL\":30.0}],\"line_alpha\":[1.0],\"line_color\":[\"white\"],\"width\":[0.8],\"x\":[\"30.0\"],\"y\":[48.0]}},\"id\":\"609fb5fc-887f-4c40-a9e6-f5cc7369e44a\",\"type\":\"ColumnDataSource\"},{\"attributes\":{},\"id\":\"fe58b3b7-b8d0-48d7-a3b3-d1c941ab69ad\",\"type\":\"CategoricalTickFormatter\"},{\"attributes\":{\"sizing_mode\":\"scale_width\",\"toolbar_location\":\"above\",\"tools\":[{\"id\":\"8631a84e-9993-4d23-bc6e-d1f0f6fdafaa\",\"type\":\"PanTool\"},{\"id\":\"027f09d6-20bf-404f-9f36-606f08d11795\",\"type\":\"WheelZoomTool\"},{\"id\":\"da6b387c-c612-46cf-95d0-9c9b194f9961\",\"type\":\"BoxZoomTool\"},{\"id\":\"1954f13f-0050-47d2-8409-4b79614b35dd\",\"type\":\"SaveTool\"},{\"id\":\"c713f4d0-5c50-44e8-940f-4cbc447be1a6\",\"type\":\"ResetTool\"},{\"id\":\"3060d8b7-1b9b-44e8-b619-91c7ae046de9\",\"type\":\"HelpTool\"}]},\"id\":\"ddf1f9f3-8355-4d93-bc89-8558f165b5a0\",\"type\":\"ToolbarBox\"},{\"attributes\":{\"source\":null},\"id\":\"5df08288-42ee-4cff-8e03-4117e8dd64e3\",\"type\":\"CDSView\"},{\"attributes\":{\"data_source\":{\"id\":\"609fb5fc-887f-4c40-a9e6-f5cc7369e44a\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"fae2ddb9-ddd1-44eb-9660-4ea8b555535e\",\"type\":\"Rect\"},\"hover_glyph\":null,\"muted_glyph\":null,\"view\":{\"id\":\"dd5e39d9-f841-4b27-951c-a6eb747be248\",\"type\":\"CDSView\"}},\"id\":\"21ac3e57-2bfa-4f35-895c-9e28e85f29e1\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"callback\":null,\"column_names\":[\"width\",\"line_color\",\"x\",\"fill_alpha\",\"line_alpha\",\"y\",\"label\",\"height\",\"color\"],\"data\":{\"VETTING_LEVEL\":[20.0],\"chart_index\":[{\"VETTING_LEVEL\":20.0}],\"color\":[\"#f22c40\"],\"fill_alpha\":[0.8],\"height\":[40.0],\"label\":[{\"VETTING_LEVEL\":20.0}],\"line_alpha\":[1.0],\"line_color\":[\"white\"],\"width\":[0.8],\"x\":[\"20.0\"],\"y\":[20.0]}},\"id\":\"dae15ab9-5495-4290-b28c-d9f3d35033f0\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"fill_alpha\":{\"field\":\"fill_alpha\"},\"fill_color\":{\"field\":\"color\"},\"height\":{\"field\":\"height\",\"units\":\"data\"},\"line_color\":{\"field\":\"line_color\"},\"width\":{\"field\":\"width\",\"units\":\"data\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"1876cc38-adf5-42c5-ac4d-c40fc6092ee9\",\"type\":\"Rect\"},{\"attributes\":{\"below\":[{\"id\":\"53b257af-e7d8-460e-986e-a84360e5295e\",\"type\":\"CategoricalAxis\"}],\"css_classes\":null,\"left\":[{\"id\":\"3bbbf15d-f670-447e-a2dc-c80c312e48ae\",\"type\":\"LinearAxis\"}],\"plot_height\":686,\"plot_width\":916,\"renderers\":[{\"id\":\"c6dee363-399b-41e8-b340-b214f1aa0ebc\",\"type\":\"BoxAnnotation\"},{\"id\":\"04728b7e-cb50-49ce-a79a-510959682a94\",\"type\":\"GlyphRenderer\"},{\"id\":\"d42cd86b-a38e-439b-b805-e99abf28534e\",\"type\":\"GlyphRenderer\"},{\"id\":\"21ac3e57-2bfa-4f35-895c-9e28e85f29e1\",\"type\":\"GlyphRenderer\"},{\"id\":\"9d07c455-8b69-4c9e-bc3d-fa789070358d\",\"type\":\"GlyphRenderer\"},{\"id\":\"cdccf352-5a37-4311-8669-e7af7bb62f7e\",\"type\":\"Legend\"},{\"id\":\"53b257af-e7d8-460e-986e-a84360e5295e\",\"type\":\"CategoricalAxis\"},{\"id\":\"3bbbf15d-f670-447e-a2dc-c80c312e48ae\",\"type\":\"LinearAxis\"},{\"id\":\"42922542-9c33-493b-8c0b-56451d0adcfb\",\"type\":\"Grid\"}],\"title\":{\"id\":\"525a758d-e93a-4b20-a3df-6d14ab543998\",\"type\":\"Title\"},\"toolbar\":{\"id\":\"58ae3922-402e-4f0f-8694-d8517891dde0\",\"type\":\"Toolbar\"},\"toolbar_location\":null,\"x_range\":{\"id\":\"907fd325-aed3-4f0f-8ca0-84fb99e86694\",\"type\":\"FactorRange\"},\"x_scale\":{\"id\":\"0bf07c51-a08e-48ec-84c3-05a0f2a6323e\",\"type\":\"CategoricalScale\"},\"y_range\":{\"id\":\"8c9127d0-eca3-4918-aee9-f81296f01b93\",\"type\":\"Range1d\"},\"y_scale\":{\"id\":\"859fb55e-6ce7-422d-8ad8-d5a02af1fa99\",\"type\":\"LinearScale\"}},\"id\":\"a07b2e40-f666-479e-9207-8f62beafda2e\",\"subtype\":\"Chart\",\"type\":\"Plot\"},{\"attributes\":{},\"id\":\"3b5f21c3-55b9-46d6-8071-283e23f1d924\",\"type\":\"BasicTicker\"},{\"attributes\":{},\"id\":\"0bf07c51-a08e-48ec-84c3-05a0f2a6323e\",\"type\":\"CategoricalScale\"},{\"attributes\":{\"children\":[{\"id\":\"a07b2e40-f666-479e-9207-8f62beafda2e\",\"subtype\":\"Chart\",\"type\":\"Plot\"}]},\"id\":\"c12c2848-e17a-4a32-9f71-e047691dc516\",\"type\":\"Row\"},{\"attributes\":{\"fill_alpha\":{\"field\":\"fill_alpha\"},\"fill_color\":{\"field\":\"color\"},\"height\":{\"field\":\"height\",\"units\":\"data\"},\"line_color\":{\"field\":\"line_color\"},\"width\":{\"field\":\"width\",\"units\":\"data\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"fae2ddb9-ddd1-44eb-9660-4ea8b555535e\",\"type\":\"Rect\"},{\"attributes\":{\"axis_label\":\"Counter\",\"formatter\":{\"id\":\"23181bce-4721-4f85-88b8-3d22d6d03cb1\",\"type\":\"BasicTickFormatter\"},\"plot\":{\"id\":\"a07b2e40-f666-479e-9207-8f62beafda2e\",\"subtype\":\"Chart\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"3b5f21c3-55b9-46d6-8071-283e23f1d924\",\"type\":\"BasicTicker\"}},\"id\":\"3bbbf15d-f670-447e-a2dc-c80c312e48ae\",\"type\":\"LinearAxis\"},{\"attributes\":{\"source\":null},\"id\":\"51c39a08-7a11-420a-ab19-f5d715e68cb6\",\"type\":\"CDSView\"},{\"attributes\":{\"axis_label\":\"Vetting_Level\",\"formatter\":{\"id\":\"fe58b3b7-b8d0-48d7-a3b3-d1c941ab69ad\",\"type\":\"CategoricalTickFormatter\"},\"major_label_orientation\":0.7853981633974483,\"plot\":{\"id\":\"a07b2e40-f666-479e-9207-8f62beafda2e\",\"subtype\":\"Chart\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"14128142-b829-4587-a96f-2cce08196d4f\",\"type\":\"CategoricalTicker\"}},\"id\":\"53b257af-e7d8-460e-986e-a84360e5295e\",\"type\":\"CategoricalAxis\"},{\"attributes\":{},\"id\":\"c713f4d0-5c50-44e8-940f-4cbc447be1a6\",\"type\":\"ResetTool\"},{\"attributes\":{\"source\":null},\"id\":\"d0d2051d-61b8-431a-9836-ed3583e88c6c\",\"type\":\"CDSView\"},{\"attributes\":{\"label\":{\"value\":\"100.0\"},\"renderers\":[{\"id\":\"9d07c455-8b69-4c9e-bc3d-fa789070358d\",\"type\":\"GlyphRenderer\"}]},\"id\":\"9cee4cf6-2116-49b8-a4b9-2e45516c27ad\",\"type\":\"LegendItem\"},{\"attributes\":{\"data_source\":{\"id\":\"dae15ab9-5495-4290-b28c-d9f3d35033f0\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"8c968091-ff11-46c2-a1ba-7e9179d55733\",\"type\":\"Rect\"},\"hover_glyph\":null,\"muted_glyph\":null,\"view\":{\"id\":\"51c39a08-7a11-420a-ab19-f5d715e68cb6\",\"type\":\"CDSView\"}},\"id\":\"d42cd86b-a38e-439b-b805-e99abf28534e\",\"type\":\"GlyphRenderer\"},{\"attributes\":{},\"id\":\"23181bce-4721-4f85-88b8-3d22d6d03cb1\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"overlay\":{\"id\":\"c6dee363-399b-41e8-b340-b214f1aa0ebc\",\"type\":\"BoxAnnotation\"}},\"id\":\"da6b387c-c612-46cf-95d0-9c9b194f9961\",\"type\":\"BoxZoomTool\"},{\"attributes\":{\"fill_alpha\":{\"field\":\"fill_alpha\"},\"fill_color\":{\"field\":\"color\"},\"height\":{\"field\":\"height\",\"units\":\"data\"},\"line_color\":{\"field\":\"line_color\"},\"width\":{\"field\":\"width\",\"units\":\"data\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"4e9c0b08-3dbc-4b5c-bc68-b1f37e02e8e5\",\"type\":\"Rect\"},{\"attributes\":{\"label\":{\"value\":\"20.0\"},\"renderers\":[{\"id\":\"d42cd86b-a38e-439b-b805-e99abf28534e\",\"type\":\"GlyphRenderer\"}]},\"id\":\"69109cb2-0e54-4261-9fa1-91a228a680f9\",\"type\":\"LegendItem\"}],\"root_ids\":[\"1718efda-ad76-4779-bfef-c6a4421900cd\"]},\"title\":\"Bokeh Application\",\"version\":\"0.12.7\"}};\n            var render_items = [{\"docid\":\"98e53efe-c5ef-4691-8e3e-cff3e18c7e42\",\"elementid\":\"af38d5b2-adff-44a4-a201-62e73dde7858\",\"modelid\":\"1718efda-ad76-4779-bfef-c6a4421900cd\"}];\n            \n            Bokeh.embed.embed_items(docs_json, render_items);\n          };\n          if (document.readyState != \"loading\") fn();\n          else document.addEventListener(\"DOMContentLoaded\", fn);\n        })();\n      },\n      function(Bokeh) {\n      }\n    ];\n  \n    function run_inline_js() {\n      \n      if ((root.Bokeh !== undefined) || (force === true)) {\n        for (var i = 0; i < inline_js.length; i++) {\n          inline_js[i].call(root, root.Bokeh);\n        }if (force === true) {\n          display_loaded();\n        }} else if (Date.now() < root._bokeh_timeout) {\n        setTimeout(run_inline_js, 100);\n      } else if (!root._bokeh_failed_load) {\n        console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n        root._bokeh_failed_load = true;\n      } else if (force !== true) {\n        var cell = $(document.getElementById(\"af38d5b2-adff-44a4-a201-62e73dde7858\")).parents('.cell').data().cell;\n        cell.output_area.append_execute_result(NB_LOAD_WARNING)\n      }\n  \n    }\n  \n    if (root._bokeh_is_loading === 0) {\n      console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n      run_inline_js();\n    } else {\n      load_libs(js_urls, function() {\n        console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n        run_inline_js();\n      });\n    }\n  }(window));\n</script>\n                    \n                \n        </div>\n    "
                    }, 
                    "metadata": {}, 
                    "output_type": "display_data"
                }
            ], 
            "cell_type": "code"
        }, 
        {
            "source": "Now, let's look at the data we have:\n\nVETTING_LEVEL is in four different statuses:\n\n    10 - HIGH\n    \n    20 - MEDIUM\n    \n    30 - LOW\n    \n    100 - Unlabeled\n\n\nPrint the total number of vetting statuses ", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "print('The number of rows labeled high is {}.'.format(AllVettingData.filter(AllVettingData['VETTING_LEVEL'] == 10).count()))\nprint('The number of rows labeled medium is {}.'.format(AllVettingData.filter(AllVettingData['VETTING_LEVEL'] == 20).count()))\nprint('The number of rows labeled low is {}.'.format(AllVettingData.filter(AllVettingData['VETTING_LEVEL'] == 30).count()))\nprint('The number of unlabeled rows is {}.'.format(AllVettingData.filter(AllVettingData['VETTING_LEVEL'] == 100).count()))", 
            "metadata": {}, 
            "execution_count": 9, 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "The number of rows labeled high is 42.\nThe number of rows labeled medium is 40.\nThe number of rows labeled low is 96.\nThe number of unlabeled rows is 907.\n"
                }
            ], 
            "cell_type": "code"
        }, 
        {
            "source": "The majority of the data has not been labeled (VETTING_LABEL=100 means unvetted).  We can not use it for our training data, so filter it out.\nPrint the total number of rows.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "LabeledVettingData=AllVettingData.filter(\"VETTING_LEVEL != 100\")\nLabeledVettingData.count()", 
            "metadata": {}, 
            "execution_count": 10, 
            "outputs": [
                {
                    "execution_count": 10, 
                    "data": {
                        "text/plain": "178"
                    }, 
                    "metadata": {}, 
                    "output_type": "execute_result"
                }
            ], 
            "cell_type": "code"
        }, 
        {
            "source": "<a id=\"engineering\"></a>\n## Feature Engineering.\n### A feature is the elements of the data that we are using in our learning.  We need to transform each one of our features into a format that SparkML can use it.\nMore about the choices for feature engineering can be found here:\nhttp://spark.apache.org/docs/2.0.0/ml-features.html#stringindexer\n\n\nThe first thing we will do is transform our labels (VETTING_LEVEL) into a format that we can use in the algorithm, and then get back to 'human readable' from in the end. The ML models require that the labels are in a column called 'label'.    The converter helps us transform these back in the end.\n\n", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "labelIndexer = StringIndexer(inputCol=\"VETTING_LEVEL\", outputCol=\"label\", handleInvalid=\"error\")\nlabelModel = labelIndexer.fit(LabeledVettingData)\nconverter = IndexToString(inputCol=\"prediction\", outputCol=\"predCategory\", labels=labelModel.labels)", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 11, 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "Next, we will process all of the features we will use. While there are a variety of choices for transforming elements, we will treat each as a String using the StringIndexer.\n\nStringIndexer is a transformer that encodes a string column to a column of indices. The indices are ordered by value frequencies, so the most frequent value gets index 0. If the input column is numeric, it is cast to string first.\n\nFor our vetting dataset, we are interested in all string based features so we will use the StringIndexer for them.  We need to use 'handleInvalid=\"skip\"' because not all values have been validated in our vetting set.  That means the algorithms will skip these records.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "occupationIndexer = StringIndexer(inputCol=\"OCCUPATION\", outputCol=\"occupationIndex\", handleInvalid=\"skip\")\ncountryIndexer = StringIndexer(inputCol=\"PASSPORT_COUNTRY_CODE\", outputCol=\"countryIndex\", handleInvalid=\"skip\")\ngenderIndexer = StringIndexer(inputCol=\"GENDER\", outputCol=\"genderIndex\", handleInvalid=\"skip\")\nyearOfBirthIndexer = StringIndexer(inputCol=\"BIRTH_YEAR\", outputCol=\"birthYearIndex\", handleInvalid=\"skip\")", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 12, 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "Now, put all of our features into a simple array using a VectorAssembler.\n\nNote that COUNTRIIES_VISITED_COUNT is already a numeric, so we can just put that in the array as is.\n", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "vecAssembler = VectorAssembler(inputCols=[\"occupationIndex\",\"countryIndex\",\"genderIndex\", \"birthYearIndex\", \"COUNTRIES_VISITED_COUNT\"], outputCol=\"features\")", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 13, 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "Normalizer will help us normalize the features into a standard frmat.  It can help us improve the behavior of the learning algorithms.\n", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "normalizer = Normalizer(inputCol=\"features\", outputCol=\"normFeatures\", p=1.0)", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 14, 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "<a id=\"model\"></a>\n## Declare the model that we want to use\n\nThe model here is Naive Bayes.  It will output each prediction into a 'prediction' column.  Naive Bayes  is a probabistic model that learns based on previous decisions.  We will take a best guess at the paramater 'smoothing'- SparkML will help us tune it later!\n\n", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\", labelCol=\"label\", predictionCol=\"prediction\")", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 15, 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "<a id=\"pipeline\"></a>\n## Setup the Pipeline\n\nThe pipeline is the guts of the algorithm that strings all the work we've done together.\n\nThe stages are run in order and the input DataFrame is transformed as it passes through each stage.   First, comes the feature transformations, then the assembler to put them togather into one DF.  We pass that into the model. \n\nIn machine learning, it is common to run a sequence of algorithms to process and learn from data, so this can get as complex as we want to make it!", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "pipeline = Pipeline(stages=[labelIndexer,occupationIndexer,countryIndexer, genderIndexer, yearOfBirthIndexer, vecAssembler, normalizer, nb, converter])", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 16, 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "<a id=\"train\"></a>\n## Train the model\n\nWe will split it into training data which is marked and test data which will be used to test the efficiency of the algorithms.\n\nIt is common to split the split up the data randomly into 70% for training and 30% for testing.  If we were to use a bigger training set, we might use an 80% / 20% split.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "train, test = LabeledVettingData.randomSplit([70.0,30.0], seed=1)\ntrain.cache()\ntest.cache()\nprint('The number of records in the training data set is {}.'.format(train.count()))\nprint('The number of rows labeled high is {}.'.format(train.filter(train['VETTING_LEVEL'] == 10).count()))\nprint('The number of rows labeled medium is {}.'.format(train.filter(train['VETTING_LEVEL'] == 20).count()))\nprint('The number of rows labeled low is {}.'.format(train.filter(train['VETTING_LEVEL'] == 30).count()))\nprint('')\n\nprint('The number of records in the test data set is {}.'.format(test.count()))\nprint('The number of rows labeled high is {}.'.format(test.filter(test['VETTING_LEVEL'] == 10).count()))\nprint('The number of rows labeled medium is {}.'.format(test.filter(test['VETTING_LEVEL'] == 20).count()))\nprint('The number of rows labeled low is {}.'.format(test.filter(test['VETTING_LEVEL'] == 30).count()))", 
            "metadata": {}, 
            "execution_count": 17, 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "The number of records in the training data set is 129.\nThe number of rows labeled high is 31.\nThe number of rows labeled medium is 34.\nThe number of rows labeled low is 64.\n\nThe number of records in the test data set is 49.\nThe number of rows labeled high is 11.\nThe number of rows labeled medium is 6.\nThe number of rows labeled low is 32.\n"
                }
            ], 
            "cell_type": "code"
        }, 
        {
            "source": " Fit the pipeline to the training data.  This will run the data through the algorithm to train it based on our labled data.\n \n<div class=\"panel-group\" id=\"accordion-3\">\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-3\" href=\"#collapse-3\">\n        Solution</a>\n      </h4>\n    </div>\n    <div id=\"collapse-3\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">Type (or copy) the following in the cell below: <br>\n          model = pipeline.fit(train)<br>\n      </div>\n    </div>\n  </div>", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "# Fit the pipeline to the training data assigning the result to a variable called 'model'.\nmodel = pipeline.fit(train)", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 18, 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "Make predictions on documents in the Test data set.  This will test the model based on the 30% data we have left in reserve.  Keep in mind that the model has not seen the data in the test data set.\n\n<div class=\"panel-group\" id=\"accordion-4\">\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-4\" href=\"#collapse-4\">\n        Solution</a>\n      </h4>\n    </div>\n    <div id=\"collapse-4\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">Type (or copy) the following in the cell below: <br>\n          predictions = model.transform(test)<br>\n      </div>\n    </div>\n  </div>", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "# Make predictions on the test data assigning the result to a variable called 'predictions'.\npredictions = model.transform(test)", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 19, 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "<a id=\"evaluate\"></a>\n## Show and Evaluate Results\n\nNote that we only got a small sample of the results back because we have a very small amount of training data. ", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "predictions.count()", 
            "metadata": {}, 
            "execution_count": 20, 
            "outputs": [
                {
                    "execution_count": 20, 
                    "data": {
                        "text/plain": "8"
                    }, 
                    "metadata": {}, 
                    "output_type": "execute_result"
                }
            ], 
            "cell_type": "code"
        }, 
        {
            "source": "SparkML has automated ways to look at result quality called Evaluators.  More information can be found here:\nhttp://spark.apache.org/docs/latest/mllib-evaluation-metrics.html\n\nFor simplicity here, we will use a a common evaluation method called Reciever Operator Characteristic.  This genenerally is used for binary classifiers, but we will use it because we only have 3 levels of prediction.\n\nThe curve is created by plotting the true positive rate against the false positive rate at various threshold settings. The ROC curve is thus the sensitivity as a function of fall-out. The area under the ROC curve is useful for comparing and selecting the best machine learning model for a given data set. A model with an area under the ROC curve score near 1 has very good performance. A model with a score near 0.5 is about as good as flipping a coin.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "evaluator = BinaryClassificationEvaluator().setLabelCol(\"label\").setMetricName(\"areaUnderROC\")\nprint('Area under the ROC curve = {}.'.format(evaluator.evaluate(predictions)))", 
            "metadata": {}, 
            "execution_count": 21, 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Area under the ROC curve = 0.6875.\n"
                }
            ], 
            "cell_type": "code"
        }, 
        {
            "source": "<a id=\"tuning\"></a>\n## Automatic Algorithm Tuning - Also Called  Hyperparameter Tuning\n\n\nSpark ML algorithms provide many hyperparameters for tuning models. These hyperparameters are distinct from the model parameters being optimized by Spark ML itself.  Hyperparameter tuning is accomplished by choosing the best set of parameters based on model performance on test data that the model was not trained with. All combinations of hyperparameters specified will be tried in order to find the one that leads to the model with the best evaluation result.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "\nFirst we will build a paramater grid to tell SparkML what to change in its testing.  Note that we are changing all the paramaters we setup in our pipeline before - the 'smoothing' in our model, and the normalizer parameter.", 
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "markdown"
        }, 
        {
            "source": "paramGrid = (ParamGridBuilder().addGrid(nb.smoothing, [0.25, 0.5, 0.75])\n                 .addGrid(normalizer.p, [1.0, 2.0]).build())", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 22, 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "Now, create a cross validator to tune the pipeline with the generated parameter grid.  Cross-validation attempts to fit the underlying estimator with user-specified combinations of parameters, cross-evaluate the fitted models, and output the best one.  ", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "cv = CrossValidator().setEstimator(pipeline).setEvaluator(evaluator).setEstimatorParamMaps(paramGrid).setNumFolds(10)", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 23, 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "Next, we will run the models through the grid we set above.  It runs Cross-evaluate the ML Pipeline to find the best model.  Note that since runs the model several times, it takes a few minutes to run.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "cvModel = cv.fit(train)\nprint('Area under the ROC curve for best fitted model = {}.'.format(evaluator.evaluate(cvModel.transform(test))))", 
            "metadata": {}, 
            "execution_count": 24, 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Area under the ROC curve for best fitted model = 0.75.\n"
                }
            ], 
            "cell_type": "code"
        }, 
        {
            "source": "Let's see what improvement we achieve by tuning the hyperparameters using cross-evaluation ", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "print('Area under the ROC curve for non-tuned model = {}.'.format(evaluator.evaluate(predictions)))\nprint('Area under the ROC curve for best fitted model = {}.'.format(evaluator.evaluate(cvModel.transform(test))))\nprint('Improvement = {0:0.2f}%'.format((evaluator.evaluate(cvModel.transform(test)) - evaluator.evaluate(predictions)) *100 / evaluator.evaluate(predictions)))", 
            "metadata": {}, 
            "execution_count": 25, 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Area under the ROC curve for non-tuned model = 0.6875.\nArea under the ROC curve for best fitted model = 0.75.\nImprovement = 9.09%\n"
                }
            ], 
            "cell_type": "code"
        }, 
        {
            "source": "We did a bit better with the new params!  Let's use \"cvModel\" instead of \"model\" below, because SparkML told us it was the best result.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "<a id=\"score\"></a>\n## Score the remaining records that were unscored, and load them into a new table in the database.\n\nFirst, we want to only get the unvetted records.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "NewVettingData=AllVettingData.filter(\"VETTING_LEVEL == 100\")", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 26, 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "Next, transform the new model with the new vetting records", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "newPreds = cvModel.transform(NewVettingData)", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 27, 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": " Show the data we have predicted and some of the fields in the data.  ", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "newPreds.select(\"UUID\", \"prediction\", \"predCategory\", \"probability\", \"NAME\", \"GENDER\", \"COUNTRIES_VISITED_COUNT\", \"PASSPORT_COUNTRY_CODE\" ).show()", 
            "metadata": {}, 
            "execution_count": 28, 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+--------------------+----------+------------+--------------------+--------------------+------+-----------------------+---------------------+\n|                UUID|prediction|predCategory|         probability|                NAME|GENDER|COUNTRIES_VISITED_COUNT|PASSPORT_COUNTRY_CODE|\n+--------------------+----------+------------+--------------------+--------------------+------+-----------------------+---------------------+\n|fc09787e-1275-435...|       1.0|        10.0|[0.05360797997827...|           Beki Hale|     F|                      4|                   GH|\n|f920719e-1def-4fc...|       0.0|        30.0|[0.99271711186883...|      Ami Washington|     F|                      5|                   GH|\n|766cc358-c850-4ba...|       1.0|        10.0|[0.00964144274960...|        Bonnie Smith|     F|                      3|                   GH|\n|3097505c-f0a1-439...|       0.0|        30.0|[0.68582782393476...|        Amber Montes|     F|                      2|                   GH|\n|9a5ad373-271c-45d...|       2.0|        20.0|[0.02096781451793...|        Marye Palmer|     F|                      7|                   GH|\n|ca350a07-f58b-429...|       0.0|        30.0|[0.96611450035493...|        Ashlee Hogan|     F|                      2|                   GH|\n|4f86014d-c3a6-42a...|       2.0|        20.0|[0.02624424220986...|        Tara Jackson|     F|                     10|                   GH|\n|8ed4b95c-6b9f-497...|       0.0|        30.0|[0.96126728895259...|      Shelly Roberts|     F|                      3|                   GH|\n|1f5f0af8-42d6-4a9...|       0.0|        30.0|[0.66083649166908...|Jennifer Kristine...|     F|                      4|                   GH|\n|7e8307a4-6dce-4e7...|       1.0|        10.0|[0.00119963562120...|        Misty Orozco|     F|                      3|                   GH|\n|36ab87be-02ff-49b...|       1.0|        10.0|[8.32213812092090...|Jessica Abigail B...|     F|                      3|                   GH|\n|664c844c-3e18-470...|       0.0|        30.0|[0.48460700021991...| Michelle Leigh Hahn|     F|                      8|                   GH|\n|3c2c6d86-caa4-471...|       2.0|        20.0|[0.00110676837062...|        Becky Miller|     F|                      7|                   GH|\n|daf3a0ae-518e-472...|       1.0|        10.0|[0.09640603791464...|     Lorie Hernandez|     F|                      2|                   GH|\n|b236f71b-d0af-4b1...|       1.0|        10.0|[0.00181197770713...| Gabby Mary Anderson|     F|                      5|                   GH|\n|9e155fa8-9dcf-417...|       1.0|        10.0|[0.21358505706763...|     Lauren Lawrence|     F|                      3|                   GH|\n|d6580ea1-cb75-44e...|       0.0|        30.0|[0.53907313984322...|           Yola Rios|     F|                      3|                   GH|\n|73f5b60f-d94e-43b...|       0.0|        30.0|[0.82393652818010...|  Beth Carrie Thomas|     F|                      1|                   GH|\n|3cac3ca7-8a59-4ec...|       2.0|        20.0|[1.23483058603913...|          Ammi Mccoy|     F|                     11|                   GH|\n|ede37c1a-8b56-48f...|       1.0|        10.0|[2.04729427280605...|   Rebekah Whitehead|     F|                      1|                   GH|\n+--------------------+----------+------------+--------------------+--------------------+------+-----------------------+---------------------+\nonly showing top 20 rows\n\n"
                }
            ], 
            "cell_type": "code"
        }, 
        {
            "source": "Remember that VETTING_LEVEL is in three different statuses:\n\n\n10- HIGH\n\n20- MEDIUM\n\n30 - LOW\n\n\nLet's print the total number of vetting statuses that we predicted.  The actual predicted data is low because we only have a few vetted records.  Remember that we had to 'skip' and features that were not in our trained data, so if we didn't have someone who was born in a certain year in our training data, we won't be able to predict a result.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "print('The number of records in the unvetted data set is {}.'.format(newPreds.count()))\nprint('The number of rows labeled high is {}.'.format(newPreds.filter(newPreds['predCategory'] == 10).count()))\nprint('The number of rows labeled medium is {}.'.format(newPreds.filter(newPreds['predCategory'] == 20).count()))\nprint('The number of rows labeled low is {}.'.format(newPreds.filter(newPreds['predCategory'] == 30).count()))", 
            "metadata": {}, 
            "execution_count": 29, 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "The number of records in the unvetted data set is 117.\nThe number of rows labeled high is 47.\nThe number of rows labeled medium is 33.\nThe number of rows labeled low is 37.\n"
                }
            ], 
            "cell_type": "code"
        }, 
        {
            "source": "<a id=\"credentials\"></a>\n## Insert the database connection credentials like we did in Lab 1\n\nClick on the cell below, then on the notebook toolbar, click the box of 1's and 0's, which is the <b>Find and Add Data</b> icon.\n\n<img alt=\"IBM Bluemix.Get started now\" src=\"https://raw.githubusercontent.com/jpatter/LMCO/master/Lab-1/images/connections-button.png\" >\n\nfind your database connection and click <b>Insert to code</b> then select the <b>Insert Credentials</b> link under the connection name to have a credentials dictionary added to the notebook. If you don't have any connections listed, refer to the PDF file in Lab-1 detailing how to add an data source.\n\n<img alt=\"IBM Bluemix.Get started now\" src=\"https://raw.githubusercontent.com/jpatter/LMCO/master/Lab-1/images/InsertToCode.PNG\" >\n\n\nConnecting to DB2 Warehouse requires the following information which are provided by the credentials dictionary inserted:\n\n    Database name\n    Host DNS name or IP address\n    Host port\n    Connection protocol\n    User ID\n    User password\n\nThe information credentials_1 will be used to build a connection string in a subsequent step. Note: it is possible that the credentials may be named credentials_2, etc. If so, simply rename to credentials_1.\n\nThe @hidden_cell directive tells DSX not to export credentials when sharing.\n", 
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "markdown"
        }, 
        {
            "source": "# The code was removed by DSX for sharing.", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 30, 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "<a id=\"write\"></a>\n## Write Results\nNow, downselect all the values we need to join in our next lab to display the results, and write to the database.  We will only load the unique ID and the prediction into our new table in DB2 Warehouse.  We'll call the table \"FEMALE_HUMAN_TRAFFICKING_ML_RESULTS\"", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "valuesToWrite= newPreds.select(\"UUID\",  \"predCategory\")\nvaluesToWrite.write.jdbc(credentials_1[\"jdbcurl\"], \"FEMALE_HUMAN_TRAFFICKING_ML_RESULTS\",\n                         properties = {\"user\" : credentials_1[\"username\"], \"password\" : credentials_1[\"password\"]},\n                         mode=\"overwrite\")", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 31, 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "<a id=\"version2\"></a>\n## Create Version \n\nSave a new version of the notebook by selecting <b>File</b> > <b>Save Version</b> \n<img alt=\"IBM Bluemix.Get started now\" src=\"https://raw.githubusercontent.com/jpatter/LMCO/master/Lab-1/images/FileOptions.PNG\" > or by selecting the <b>Versions</b> icon and selecting <b>Save Version</b>. <img alt=\"IBM Bluemix.Get started now\" src=\"https://raw.githubusercontent.com/jpatter/LMCO/master/Lab-1/images/versions-button.png\" ><br>\nYou can have up to ten (10) versions of a notebook.   Notebook versions are saved in a FIFO manner.", 
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "markdown"
        }, 
        {
            "source": "<a id=\"schedule\"></a>\n## Schedule Job\nYou can schedule a notebook version to run at specified intervals.   If a notebook version does not yet exist, one will be created for you.  If the notebook kernel was stopped when scheduled to run, it will be started.\n\nTo schedule a notebook, select the <b>Schedule</b> icon.\n\n<img alt=\"IBM Bluemix.Get started now\" src=\"https://raw.githubusercontent.com/jpatter/LMCO/master/Lab-1/images/schedule-button.png\" >\n\nGive a name to the job and pick the time period to run it.   All time periods are for the timezone of the <b>browser</b> NOT the timezone of the server where the notebook is running.\n\n<img alt=\"IBM Bluemix.Get started now\" src=\"https://raw.githubusercontent.com/jpatter/LMCO/master/Lab-1/images/Schedule-Window.PNG\" >", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "## Download notebook\n\nNotebooks can be downloaded in notebook (.ipynb), Python (.py), HTML (.html), markdown (.md) or reST (.rst) format.  Use <b>File</b> > <b>Download as</b> to download the notebook in any of the formats.\n\n<img alt=\"IBM Bluemix.Get started now\" src=\"https://raw.githubusercontent.com/jpatter/LMCO/master/Lab-1/images/FileOptions.PNG\" >", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "<a id=\"revert\"></a>\n## Revert to version \nRevert to the version you saved at the beginning of this lab.   There are two ways to do this.   First, select <b>File</b> > <b>Revert to Version</b> and choose the version you created at the beginning of the lab (versions are timestamped).\n<img alt=\"IBM Bluemix.Get started now\" src=\"https://raw.githubusercontent.com/jpatter/LMCO/master/Lab-1/images/FileOptions.PNG\" >\n\nThe second way is to select the <b>Versions</b> icon \n<img alt=\"IBM Bluemix.Get started now\" src=\"https://raw.githubusercontent.com/jpatter/LMCO/master/Lab-1/images/versions-button.png\" ><br>\nand then select the version you wish to revert to.   You can also delete versions from here.\n<img alt=\"IBM Bluemix.Get started now\" src=\"https://raw.githubusercontent.com/jpatter/LMCO/master/Lab-1/images/Versions.PNG\" >", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "<a id=\"help\"></a>\n## Even more help\n\nSelect the <b>Find Resources in the Community</b> link to display a search bar, documentation hotlinks, and a link to Stack Overflow's Data Science Experience section.\n\n<img alt=\"IBM Bluemix.Get started now\" src=\"https://raw.githubusercontent.com/jpatter/LMCO/master/Lab-1/images/community-button.png\" >\n\n<img alt=\"IBM Bluemix.Get started now\" src=\"https://raw.githubusercontent.com/jpatter/LMCO/master/Lab-1/images/Community-Resources.PNG\" >", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": null, 
            "outputs": [], 
            "cell_type": "code"
        }
    ], 
    "nbformat_minor": 1
}